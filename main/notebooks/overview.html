<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Overview of MaskedTensors &mdash; MaskedTensor  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Resolving NaN Grad" href="nan_grad.html" />
    <link rel="prev" title="MaskedTensor" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MaskedTensor
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Tutorials:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overview of MaskedTensors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basic-masking-semantics">Basic masking semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#resolving-or-revisiting-some-issues">Resolving or revisiting some issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">1369</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">21987</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nan_grad.html">Resolving NaN Grad</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MaskedTensor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Overview of MaskedTensors</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/overview.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="overview-of-maskedtensors">
<h1>Overview of MaskedTensors<a class="headerlink" href="#overview-of-maskedtensors" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://github.com/pytorch/maskedtensor/tree/main/docs/notebooks/overview.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">if</span> <span class="s2">&quot;1.11.0&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">:</span>
    <span class="o">!</span>pip uninstall --y torch
    <span class="o">!</span>pip install torch -f https://download.pytorch.org/whl/test/cu102/torch_test.html --pre
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install -i https://test.pypi.org/simple/ maskedtensor
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Looking in indexes: https://test.pypi.org/simple/
Requirement already satisfied: maskedtensor in /data/home/georgeqi/miniconda/envs/mt_release_0.1.0/lib/python3.8/site-packages (0.1.0)
Requirement already satisfied: torch&lt;1.12,&gt;=1.11 in /data/home/georgeqi/miniconda/envs/mt_release_0.1.0/lib/python3.8/site-packages (from maskedtensor) (1.11.0+cpu)
Requirement already satisfied: typing-extensions in /data/home/georgeqi/miniconda/envs/mt_release_0.1.0/lib/python3.8/site-packages (from torch&lt;1.12,&gt;=1.11-&gt;maskedtensor) (3.10.0.2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import factory function</span>
<span class="kn">from</span> <span class="nn">maskedtensor</span> <span class="kn">import</span> <span class="n">masked_tensor</span>
<span class="kn">from</span> <span class="nn">maskedtensor</span> <span class="kn">import</span> <span class="n">as_masked_tensor</span>
</pre></div>
</div>
</div>
</div>
<section id="basic-masking-semantics">
<h2>Basic masking semantics<a class="headerlink" href="#basic-masking-semantics" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First example of addition</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">5.</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">])</span>
<span class="n">m0</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">m0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m0</span> <span class="o">+</span> <span class="n">m0</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
  <span class="c1"># For now the masks must match. We treat them like shapes.</span>
  <span class="c1"># We can relax this later on, but should have a good reason for it.</span>
  <span class="c1"># We&#39;ll revisit this once we have reductions.</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">m0</span> <span class="o">+</span> <span class="n">m1</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>masked_tensor(
  [  0.0000,   1.0000,       --,   3.0000,       --]
)
masked_tensor(
  [      --,       --,   2.0000,       --,   4.0000]
)
masked_tensor(
  [  1.0000,   0.5403,       --,  -0.9900,       --]
)
masked_tensor(
  [  0.0000,   2.0000,       --,   6.0000,       --]
)
Input masks must match. If you need support for this, please open an issue on Github.
</pre></div>
</div>
</div>
</div>
<p>NumPy’s MaskedArray implements intersection semantics here. If one of two elements are masked out the resulting element will be masked out as well. Note that MaskedArray’s factory function inverts the mask (similar to torch.nn.MHA). For MaskedTensor we’d apply the logical_and operator to both masks during a binary operation to get the semantics NumPy has. Since NumPy stores the inverted mask they <a class="reference external" href="https://github.com/numpy/numpy/blob/68299575d8595d904aff6f28e12d21bf6428a4ba/numpy/ma/core.py#L1016-L1024">apply the logical_or operator</a>. But to repeat this point we suggest to not support addition between MaskedTensors with masks that don’t match. See the section on reductions for why we should have good reasons for this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">npm0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">npm1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">npm0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">npm1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">npm0</span> <span class="o">+</span> <span class="n">npm1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.0 1.0 -- 3.0 --]
[-- -- 2.0 -- 4.0]
[-- -- -- -- --]
</pre></div>
</div>
</div>
</div>
<p>MaskedTensor also supports these semantics by giving access to the masks and conveniently converting a MaskedTensor to a Tensor with masked values filled in with a particular value.</p>
<p>NumPy of course has the opportunity to avoid addition altogether in this case by check whether any results are not masked, but <a class="reference external" href="https://github.com/numpy/numpy/blob/68299575d8595d904aff6f28e12d21bf6428a4ba/numpy/ma/core.py#L1013">chooses not to</a>. Presumably it’s more expensive to allreduce the mask every time to avoid the binary addition of the data in this case.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m0t</span> <span class="o">=</span> <span class="n">m0</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">m1t</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">m2t</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">m0t</span> <span class="o">+</span> <span class="n">m1t</span><span class="p">,</span> <span class="n">m0</span><span class="o">.</span><span class="n">mask</span><span class="p">()</span> <span class="o">&amp;</span> <span class="n">m1</span><span class="o">.</span><span class="n">mask</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m0t</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m1t</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m2t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 1., 0., 3., 0.])
tensor([0., 0., 2., 0., 4.])
masked_tensor(
  [      --,       --,       --,       --,       --]
)
</pre></div>
</div>
</div>
</div>
<p>Example of printing a 2d MaskedTensor and setup for reductions below</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ -2.,   8.,  21.],
        [  6., -11.,  -5.],
        [ -7.,  14.,   2.],
        [  1.,  -6.,   1.],
        [-12.,  -4.,   0.],
        [  0.,   0., -11.],
        [  1.,   5.,   6.],
        [  4.,   8.,  -5.]])
tensor([[False,  True,  True],
        [False,  True,  True],
        [ True,  True, False],
        [ True, False,  True],
        [False,  True, False],
        [ True,  True, False],
        [False,  True, False],
        [False, False,  True]])
masked_tensor(
  [
    [      --,   8.0000,  21.0000],
    [      --, -11.0000,  -5.0000],
    [ -7.0000,  14.0000,       --],
    [  1.0000,       --,   1.0000],
    [      --,  -4.0000,       --],
    [  0.0000,   0.0000,       --],
    [      --,   5.0000,       --],
    [      --,       --,  -5.0000]
  ]
)
</pre></div>
</div>
</div>
</div>
<p>Reduction semantics based on https://github.com/pytorch/rfcs/pull/27</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>masked_tensor(
  [ 29.0000, -16.0000,   7.0000,   2.0000,  -4.0000,   0.0000,   5.0000,  -5.0000]
)
masked_tensor(
  [ 14.5000,  -8.0000,   3.5000,   1.0000,  -4.0000,   0.0000,   5.0000,  -5.0000]
)
masked_tensor(
  [168.0000,  55.0000, -98.0000,   1.0000,  -4.0000,   0.0000,   5.0000,  -5.0000]
)
masked_tensor(
  [  8.0000, -11.0000,  -7.0000,   1.0000,  -4.0000,   0.0000,   5.0000,  -5.0000]
)
masked_tensor(
  [ 21.0000,  -5.0000,  14.0000,   1.0000,  -4.0000,   0.0000,   5.0000,  -5.0000]
)
</pre></div>
</div>
</div>
</div>
<p>Now that we have reductions, let’s revisit as to why we’ll probably want to have a good reason to allow addition of MaskedTensors with different masks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">10.</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">data1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">10.</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">10</span>
<span class="n">mask0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]])</span>
<span class="n">mask1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">]])</span>

<span class="n">npm0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">data0</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="p">(</span><span class="n">mask0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">npm1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">data1</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="p">(</span><span class="n">mask1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">npm0:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">npm0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">npm1:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">npm1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(npm0 + npm1).sum(0):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">npm0</span> <span class="o">+</span> <span class="n">npm1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">npm0.sum(0) + npm1.sum(0):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">npm0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">npm1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(data0 + data1).sum(0):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">data0</span> <span class="o">+</span> <span class="n">data1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(data0 + data1).sum(0):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">data0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">data1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>npm0:
 [[-- -- 2.0 3.0 4.0]
 [5.0 6.0 7.0 -- --]]

npm1:
 [[10.0 11.0 12.0 -- --]
 [-- -- 17.0 18.0 19.0]]

(npm0 + npm1).sum(0):
 [-- -- 38.0 -- --]

npm0.sum(0) + npm1.sum(0):
 [15.0 17.0 38.0 21.0 23.0]

(data0 + data1).sum(0):
 tensor([30., 34., 38., 42., 46.])

(data0 + data1).sum(0):
 tensor([30., 34., 38., 42., 46.])
</pre></div>
</div>
</div>
</div>
<p>Sum and addition should be associative. However with NumPy’s semantics we allow them not to be. Instead of allowing these semantics, at least in the case of addition and sum, we could ask the user to fill the MaskedTensor’s undefined elements with 0 values or as in the MaskedTensor addition examples above be very specific about the semantics used.</p>
<p>While it’s obviously possible to support this, I think we should cover other operators first and really make sure we can’t avoid this behavior via other means.</p>
<p>Now let’s print some higher dimensional MaskedTensors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>masked_tensor(
  [
    [
      [      --,       --,   3.7374],
      [ -0.7823,       --,  -2.7896],
      [      --,  -3.7006,  -3.1877],
      [ -1.9477,       --,   9.4375],
      [      --,       --,       --]
    ],
    [
      [      --,  -0.9693,  -4.7567],
      [      --,       --,  -0.0910],
      [      --,   4.0244,       --],
      [      --,  -1.1483,   5.9897],
      [ -6.4715,       --,   6.6507]
    ],
    [
      [ -2.7378,   1.3096,  -1.8739],
      [      --,       --,  -1.4943],
      [ -3.2081,       --,       --],
      [ 11.0149,       --,   8.7485],
      [      --,       --,  -3.0937]
    ],
    [
      [      --,  -5.7733,       --],
      [  6.0305,  -0.1201,  -1.9697],
      [  5.5723,   2.9670,       --],
      [      --,       --,   2.2725],
      [  2.5047,   4.6743,       --]
    ]
  ]
)
</pre></div>
</div>
</div>
</div>
<p>Example of indexing and advanced indexing</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">mask</span><span class="p">()])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>masked_tensor(
  [
    [      --,       --,   3.7374],
    [ -0.7823,       --,  -2.7896],
    [      --,  -3.7006,  -3.1877],
    [ -1.9477,       --,   9.4375],
    [      --,       --,       --]
  ]
)
masked_tensor(
  [
    [
      [      --,       --,   3.7374],
      [ -0.7823,       --,  -2.7896],
      [      --,  -3.7006,  -3.1877],
      [ -1.9477,       --,   9.4375],
      [      --,       --,       --]
    ],
    [
      [ -2.7378,   1.3096,  -1.8739],
      [      --,       --,  -1.4943],
      [ -3.2081,       --,       --],
      [ 11.0149,       --,   8.7485],
      [      --,       --,  -3.0937]
    ]
  ]
)
masked_tensor(
  [  3.7374,  -0.7823,  -2.7896,  -3.7006,  -3.1877,  -1.9477,   9.4375,  -0.9693,  -4.7567,  -0.0910,   4.0244,  -1.1483,   5.9897,  -6.4715,   6.6507,  -2.7378,   1.3096,  -1.8739,  -1.4943,  -3.2081,  11.0149,   8.7485,  -3.0937,  -5.7733,   6.0305,  -0.1201,  -1.9697,   5.5723,   2.9670,   2.2725,   2.5047,   4.6743]
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>
<span class="c1"># Sum needs custom autograd, since the mask of the input should be maintained</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;s: &quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="n">s</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;m.grad: &quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

<span class="c1"># sum needs to return a scalar MaskedTensor because the input might be fully masked</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;s: &quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="n">s</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;m.grad: &quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>masked_tensor(
  [
    [
      [      --,       --,  -0.5084],
      [  6.7935, -15.3725,       --]
    ],
    [
      [      --,   1.2078,       --],
      [  6.5820,       --,  -1.6679]
    ]
  ]
)
s:  masked_tensor( -2.9655, True)
m.grad:  masked_tensor(
  [
    [
      [      --,       --,   1.0000],
      [  1.0000,   1.0000,       --]
    ],
    [
      [      --,   1.0000,       --],
      [  1.0000,       --,   1.0000]
    ]
  ]
)

 masked_tensor(
  [
    [
      [      --,       --,       --],
      [      --,       --,       --]
    ],
    [
      [      --,       --,       --],
      [      --,       --,       --]
    ]
  ]
)
s:  masked_tensor(--, False)
m.grad:  masked_tensor(
  [
    [
      [      --,       --,       --],
      [      --,       --,       --]
    ],
    [
      [      --,       --,       --],
      [      --,       --,       --]
    ]
  ]
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grad of multiplication of MaskedTensor and Tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># The mask broadcast in the sense that the result is masked.</span>
<span class="c1"># In general a MaskedTensor is considered a generalization of Tensor&#39;s shape.</span>
<span class="c1"># The mask is a more complex, higher dimensional shape and thus the Tensor</span>
<span class="c1"># broadcasts to it. I&#39;d love to find a more rigorous definition of this.</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x * y:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">x.grad: &quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># The regular torch.Tensor now has a MaskedTensor grad</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.grad: &quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x:
 masked_tensor(
  [  3.0000,       --]
)
y:
 tensor([2., 1.], requires_grad=True)
x * y:
 masked_tensor(
  [  6.0000,       --]
)

x.grad:  masked_tensor(
  [  2.0000,       --]
)
y.grad:  masked_tensor(
  [  3.0000,       --]
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># is_contiguous doesn&#39;t work</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">mt</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="n">mt</span> <span class="o">=</span> <span class="n">mt</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">mt</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="n">mt</span> <span class="o">=</span> <span class="n">mt</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mt</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="n">mt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">mt</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mt</span><span class="o">.</span><span class="n">masked_data</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="n">mt</span><span class="o">.</span><span class="n">masked_data</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">mt</span><span class="o">.</span><span class="n">masked_data</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
<span class="n">mt</span> <span class="o">=</span> <span class="n">mt</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">mt</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mt</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="n">mt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">mt</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
<span class="n">mt</span> <span class="o">=</span> <span class="n">mt</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mt</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="n">mt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">mt</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True torch.Size([2, 1, 2]) (2, 2, 1)
True torch.Size([2, 1, 2]) (2, 4, 1)
True torch.Size([2, 1, 2]) (2, 2, 1)
True torch.Size([2, 1, 2]) (2, 2, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Because .contiguous doesn&#39;t work we need to modify view to use reshape instead</span>
<span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">mt</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mt</span> <span class="o">=</span> <span class="n">mt</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="n">mt</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="resolving-or-revisiting-some-issues">
<h2>Resolving or revisiting some issues<a class="headerlink" href="#resolving-or-revisiting-some-issues" title="Permalink to this headline"></a></h2>
<p>In some cases MaskedTensors can provide a resolution, in others it can provide an alternative or best case more principled approach.</p>
<section id="id1">
<h3><a class="reference external" href="https://github.com/pytorch/pytorch/issues/1369">1369</a><a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>This issue discussed the additional lines of code that were introduce while writing “sparse” semantics for Adagrad. But really the code doesn’t use sparsity as a compression and optimization technique, it wants to use masked semantics. We worked around this by introducing one-off semantics and operators that encode this behavior while forcing users to be aware of storage details such as indices and values. Let’s look at the current implementation of <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py">Adagrad</a> <a class="reference external" href="https://github.com/pytorch/pytorch/blob/6c2f235d368b697072699e5ca9485fd97d0b9bcc/torch/optim/_functional.py#L16-L51">(functional)</a> to illustrate that.</p>
<p>In particular we’ll point out when sparsity is used as a semantic extension, i.e. unspecified values are not zero and when it is just used to compress zeros. We’ll also compare and contrast this with equivalent code written using MaskedTensor. In the end the code snippets are repeat without additional comments to show the difference in brevity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_make_sparse</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">grad_indices</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">values</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">grad_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

<span class="c1"># We don&#39;t support sparse gradients</span>
<span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">state_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="c1"># initial value for state sum</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;param:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;grad:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;state_sum:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_sum</span><span class="p">)</span>

<span class="c1"># Some hyperparameters</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-10</span>
<span class="n">clr</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>param:
 tensor([[0., 1., 2., 3.],
        [4., 5., 6., 7.]])
grad:
 tensor([[0., 0., 3., 0.],
        [4., 0., 5., 0.]])
state_sum:
 tensor([[0.5000, 0.5000, 0.5000, 0.5000],
        [0.5000, 0.5000, 0.5000, 0.5000]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">state_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="c1"># initial value for state sum</span>
<span class="nb">print</span><span class="p">(</span><span class="n">state_sum</span><span class="p">)</span>

<span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>  <span class="c1"># the update is non-linear so indices must be unique</span>
<span class="n">grad_indices</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
<span class="n">grad_values</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

<span class="c1"># pow(2) has the same semantics for both sparse and dense memory layouts since</span>
<span class="c1"># 0^2 is zero</span>
<span class="n">state_sum</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">_make_sparse</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad_indices</span><span class="p">,</span> <span class="n">grad_values</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
<span class="c1"># We take care to make std sparse, even though state_sum clearly is not.</span>
<span class="c1"># This means that we&#39;re only applying the gradient to parts of the state_sum</span>
<span class="c1"># for which it is specified. This even drives the point home a lot more that</span>
<span class="c1"># the passed gradient is not sparse, but masked. </span>
<span class="n">std</span> <span class="o">=</span> <span class="n">state_sum</span><span class="o">.</span><span class="n">sparse_mask</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;state_sum:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_sum</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;std:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">std</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.5000, 0.5000, 0.5000, 0.5000],
        [0.5000, 0.5000, 0.5000, 0.5000]])
state_sum:
 tensor([[ 0.5000,  0.5000,  9.5000,  0.5000],
        [16.5000,  0.5000, 25.5000,  0.5000]])
std:
 tensor([[ 0.0000,  0.0000,  9.5000,  0.0000],
        [16.5000,  0.0000, 25.5000,  0.0000]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is where we have a very important divergence. The addition of eps</span>
<span class="c1"># should technically be applied to all values, but instead is only applied to</span>
<span class="c1"># specified values. Here we&#39;re using sparsity as a semantic extension and</span>
<span class="c1"># to enforce a certain pattern of defined and undefined values. If parts</span>
<span class="c1"># of the values of the gradient are zero they are still included if materialized.</span>
<span class="c1"># Even though they could be compressed by other sparse storage layouts.</span>
<span class="c1"># This is technically quite brittle even though someone could argue that eps is</span>
<span class="c1"># always very small.</span>

<span class="c1"># More so an implementation add_ for sparsity as a storage layout and compression</span>
<span class="c1"># scheme should cause densification, but we force it not to. For this one-off</span>
<span class="c1"># case it is fine until we want to introduce new compression schemes such as</span>
<span class="c1"># CSR, BSR or 2:4 block sparsity. We&#39;ll then need to introduce separate Tensor</span>
<span class="c1"># types for each and write variations for gradients compressed using different</span>
<span class="c1"># storage formats.</span>

<span class="c1"># We currently dodge all these concerns using the private method values.</span>
<span class="n">std_values</span> <span class="o">=</span> <span class="n">std</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt_</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>

<span class="c1"># We currently don&#39;t support div for sparse Tensors because zero / zero is</span>
<span class="c1"># not well defined. For a MaskedTensor undefined / undefined is undefined.</span>
<span class="n">param</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">_make_sparse</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad_indices</span><span class="p">,</span> <span class="n">grad_values</span> <span class="o">/</span> <span class="n">std_values</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">clr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;param:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>param:
 tensor([[0.0000, 1.0000, 1.9027, 3.0000],
        [3.9015, 5.0000, 5.9010, 7.0000]])
</pre></div>
</div>
</div>
</div>
<p>We’ve been conflating sparsity as an optimization with sparsity as a semantic extension to PyTorch. MaskedTensor proposes to call the semantic extension through sparsity masked. Currently we can’t have dense semantics with sparse storage or masked semantics with dense storage. MaskedTensor fixes that because it separates the storage from the semantics. Let’s look at above example using a masked gradient.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Of course we can add sparsity as a storage layout for MaskedTensor which</span>
<span class="c1"># compresses repeated undefined values. We can recycle SparseTensor and SparseCSR</span>
<span class="c1"># by setting data and mask to an instance of each that share indices.</span>
<span class="c1"># However, ideally we&#39;d just have regular torch.Tensors with a sparse layout</span>
<span class="c1"># and use those to back MaskedTensor.</span>
<span class="n">masked_grad</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">to_dense</span><span class="p">(),</span> <span class="n">grad</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;masked_grad:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">masked_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>masked_grad:
 masked_tensor(
  [
    [      --,       --,   3.0000,       --],
    [  4.0000,       --,   5.0000,       --]
  ]
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an entirely new set of parameters to avoid errors</span>
<span class="n">param2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">state_sum2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="c1"># initial value for state sum</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is an excellent example of why to_tensor is important. We don&#39;t</span>
<span class="c1"># want to propagate the mask to state_sum, but still maintain the compression.</span>
<span class="c1"># to_tensor could eventually return a Tensor with sparse layout for the</span>
<span class="c1"># special value of zero or first require explicit densification if it can&#39;t</span>
<span class="c1"># maintain the layout.</span>

<span class="c1"># This is also a value proposition for sparsity</span>
<span class="c1"># as a separate layout and a SparseTensor with dense semantics. MaskedTensor</span>
<span class="c1"># can be much simpler without having to introduce complex maske union and intersection</span>
<span class="c1"># semantics for binary operations.</span>
<span class="n">state_sum2</span> <span class="o">=</span> <span class="n">state_sum2</span> <span class="o">+</span> <span class="n">masked_grad</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># We can eventually construct a masked std backed by a sparse layout</span>
<span class="n">std2</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">state_sum2</span><span class="p">,</span> <span class="n">masked_grad</span><span class="o">.</span><span class="n">mask</span><span class="p">())</span> <span class="c1">#, layout=torch.layout.coo)</span>
<span class="c1"># Let&#39;s print both this version and the regular version for easier comparison</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;state_sum:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_sum</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;std:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;state_sum2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_sum2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;std2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">std2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>state_sum:
 tensor([[ 0.5000,  0.5000,  9.5000,  0.5000],
        [16.5000,  0.5000, 25.5000,  0.5000]])
std:
 tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3.0822, 4.0620, 5.0498]),
       size=(2, 4), nnz=3, layout=torch.sparse_coo)
state_sum2:
 tensor([[ 0.5000,  0.5000,  9.5000,  0.5000],
        [16.5000,  0.5000, 25.5000,  0.5000]])
std2:
 masked_tensor(
  [
    [      --,       --,   9.5000,       --],
    [ 16.5000,       --,  25.5000,       --]
  ]
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We can add support for in-place operations later. Notice how this doesn&#39;t</span>
<span class="c1"># need to access any storage internals and is in general a lot shorter</span>
<span class="n">std2</span> <span class="o">=</span> <span class="n">std2</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;std:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;std2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">std2</span><span class="p">)</span>

<span class="c1"># to_tensor ideally eventually returns a torch.Tensor with sparse layout</span>
<span class="c1"># but would currently return a SparseTensor.</span>
<span class="n">param2</span> <span class="o">=</span> <span class="n">param2</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">masked_grad</span> <span class="o">/</span> <span class="n">std2</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">clr</span><span class="p">)</span>

<span class="c1"># The final results is the same</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;param:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;param2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>std:
 tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3.0822, 4.0620, 5.0498]),
       size=(2, 4), nnz=3, layout=torch.sparse_coo)
std2:
 masked_tensor(
  [
    [      --,       --,   3.0822,       --],
    [  4.0620,       --,   5.0498,       --]
  ]
)
param:
 tensor([[0.0000, 1.0000, 1.9027, 3.0000],
        [3.9015, 5.0000, 5.9010, 7.0000]])
param2:
 tensor([[0.0000, 1.0000, 1.9027, 3.0000],
        [3.9015, 5.0000, 5.9010, 7.0000]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # For reference, this is the regular, dense code path without masked gradients or sparsity</span>
<span class="c1"># state_sum.addcmul_(grad, grad, value=1)</span>
<span class="c1"># std = state_sum.sqrt().add_(eps)</span>
<span class="c1"># param.addcdiv_(grad, std, value=-clr)</span>

<span class="c1"># Compare this to the original for sparse</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>  <span class="c1"># the update is non-linear so indices must be unique</span>
<span class="n">grad_indices</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
<span class="n">grad_values</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

<span class="n">state_sum</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">_make_sparse</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad_indices</span><span class="p">,</span> <span class="n">grad_values</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">state_sum</span><span class="o">.</span><span class="n">sparse_mask</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<span class="n">std_values</span> <span class="o">=</span> <span class="n">std</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt_</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<span class="n">param</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">_make_sparse</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad_indices</span><span class="p">,</span> <span class="n">grad_values</span> <span class="o">/</span> <span class="n">std_values</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">clr</span><span class="p">)</span>

<span class="c1"># All in all MaskedTensor minimizes the code to the follwing snippet</span>
<span class="n">state_sum2</span> <span class="o">=</span> <span class="n">state_sum2</span> <span class="o">+</span> <span class="n">masked_grad</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">std2</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">state_sum2</span><span class="p">,</span> <span class="n">masked_grad</span><span class="o">.</span><span class="n">mask</span><span class="p">())</span> <span class="c1">#, layout=torch.layout.coo)</span>
<span class="n">std2</span> <span class="o">=</span> <span class="n">std2</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<span class="n">param2</span> <span class="o">=</span> <span class="n">param2</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">masked_grad</span> <span class="o">/</span> <span class="n">std2</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">clr</span><span class="p">)</span>

<span class="c1"># We ran this code again so let&#39;s check that the results again match</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;param:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;param2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>param:
 tensor([[0.0000, 1.0000, 1.8329, 3.0000],
        [3.8314, 5.0000, 5.8306, 7.0000]])
param2:
 tensor([[0.0000, 1.0000, 1.8329, 3.0000],
        [3.8314, 5.0000, 5.8306, 7.0000]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h3><a class="reference external" href="https://github.com/pytorch/pytorch/issues/21987">21987</a><a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>Was closed by inclusion into <a class="reference external" href="https://github.com/pytorch/pytorch/issues/61474">Implement missing torch.nan* operators</a>. This proposes an alternative, which is to use masked tensors instead of introducing additional operators. Since nanmean <a class="reference external" href="https://github.com/pytorch/pytorch/issues/21987">has already landed</a> we can use it as a comparison point.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">masked_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([nan,  1.,  4.,  9., nan,  5., 12., 21., nan,  9., 20., 33., nan, 13.,
        28., 45., nan, 17., 36., 57., nan, 21., 44., 69., nan, 25., 52., 81.,
        nan, 29., 60., 93.])
tensor(32.6667)
masked_tensor( 32.6667, True)
</pre></div>
</div>
</div>
</div>
<p>MaskedTensor can further support reduction when fully masked out, as would be the case when a given Tensor is completetely nan. nanmean on the other hand returns nan when the input is entirely nan.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">masked_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan])
tensor(nan)
masked_tensor(--, False)
</pre></div>
</div>
</div>
</div>
<p>Further <a class="reference external" href="https://github.com/pytorch/pytorch/issues/63870">some users</a> already want to use nan reductions to encode masked semantics.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="MaskedTensor" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="nan_grad.html" class="btn btn-neutral float-right" title="Resolving NaN Grad" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, PyTorch.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>