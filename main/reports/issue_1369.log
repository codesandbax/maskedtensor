Traceback (most recent call last):
  File "/home/runner/.local/lib/python3.8/site-packages/jupyter_cache/executors/utils.py", line 51, in single_nb_execution
    executenb(
  File "/home/runner/.local/lib/python3.8/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/home/runner/.local/lib/python3.8/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/home/runner/.local/lib/python3.8/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/usr/lib/python3.8/asyncio/base_events.py", line 616, in run_until_complete
    return future.result()
  File "/home/runner/.local/lib/python3.8/site-packages/nbclient/client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "/home/runner/.local/lib/python3.8/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/home/runner/.local/lib/python3.8/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# This is an excellent example of why to_tensor is important. We don't
# want to propagate the mask to state_sum, but still maintain the compression.
# to_tensor could eventually return a Tensor with sparse layout for the
# special value of zero or first require explicit densification if it can't
# maintain the layout.

# This is also a value proposition for sparsity
# as a separate layout and a SparseTensor with dense semantics. MaskedTensor
# can be much simpler without having to introduce complex maske union and intersection
# semantics for binary operations.

state_sum2 = state_sum2 + masked_grad.pow(2).to_tensor(0)
# We can eventually construct a masked std backed by a sparse layout
std2 = masked_tensor(state_sum2, masked_grad.mask()) #, layout=torch.layout.coo)
# Let's print both this version and the regular version for easier comparison
print("state_sum:\n", state_sum)
print("std:\n", std)
print("state_sum2:\n", state_sum2)
print("std2:\n", std2)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
Input [0;32mIn [8][0m, in [0;36m<cell line: 12>[0;34m()[0m
[1;32m      1[0m [38;5;66;03m# This is an excellent example of why to_tensor is important. We don't[39;00m
[1;32m      2[0m [38;5;66;03m# want to propagate the mask to state_sum, but still maintain the compression.[39;00m
[1;32m      3[0m [38;5;66;03m# to_tensor could eventually return a Tensor with sparse layout for the[39;00m
[0;32m   (...)[0m
[1;32m      9[0m [38;5;66;03m# can be much simpler without having to introduce complex maske union and intersection[39;00m
[1;32m     10[0m [38;5;66;03m# semantics for binary operations.[39;00m
[0;32m---> 12[0m state_sum2 [38;5;241m=[39m state_sum2 [38;5;241m+[39m [43mmasked_grad[49m[38;5;241;43m.[39;49m[43mpow[49m[43m([49m[38;5;241;43m2[39;49m[43m)[49m[38;5;241m.[39mto_tensor([38;5;241m0[39m)
[1;32m     13[0m [38;5;66;03m# We can eventually construct a masked std backed by a sparse layout[39;00m
[1;32m     14[0m std2 [38;5;241m=[39m masked_tensor(state_sum2, masked_grad[38;5;241m.[39mmask()) [38;5;66;03m#, layout=torch.layout.coo)[39;00m

File [0;32m~/.local/lib/python3.8/site-packages/maskedtensor/core.py:218[0m, in [0;36mMaskedTensor.__torch_function__[0;34m(cls, func, types, args, kwargs)[0m
[1;32m    216[0m logging[38;5;241m.[39mdebug([38;5;124m"[39m[38;5;124mtf redispatching to td[39m[38;5;124m"[39m)
[1;32m    217[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39m_C[38;5;241m.[39mDisableTorchFunction():
[0;32m--> 218[0m     ret [38;5;241m=[39m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    219[0m     [38;5;28;01mif[39;00m func [38;5;129;01min[39;00m get_default_nowrap_functions():
[1;32m    220[0m         [38;5;28;01mreturn[39;00m ret

File [0;32m~/.local/lib/python3.8/site-packages/maskedtensor/core.py:319[0m, in [0;36mMaskedTensor.__torch_dispatch__[0;34m(cls, func, types, args, kwargs)[0m
[1;32m    317[0m     func(data, get_data(args[[38;5;241m1[39m]))
[1;32m    318[0m     [38;5;28;01mreturn[39;00m args[[38;5;241m0[39m]
[0;32m--> 319[0m [38;5;28;01mif[39;00m func [38;5;129;01min[39;00m [[43mtorch[49m[38;5;241;43m.[39;49m[43mops[49m[38;5;241;43m.[39;49m[43maten[49m[38;5;241;43m.[39;49m[43m_s_where[49m]:
[1;32m    320[0m     [38;5;28;01massert[39;00m [38;5;28mlen[39m(kwargs) [38;5;241m==[39m [38;5;241m0[39m
[1;32m    321[0m     [38;5;28;01massert[39;00m [38;5;28mlen[39m(args) [38;5;241m==[39m [38;5;241m3[39m

File [0;32m~/.local/lib/python3.8/site-packages/torch/_ops.py:191[0m, in [0;36m__getattr__[0;34m(self, op_name)[0m
[1;32m    188[0m [38;5;28;01mdef[39;00m [38;5;21m__getattr__[39m([38;5;28mself[39m, name):
[1;32m    189[0m     [38;5;66;03m# Here we are creating `torch.ops.my_namespace`[39;00m
[1;32m    190[0m     namespace [38;5;241m=[39m _OpNamespace(name)
[0;32m--> 191[0m     [38;5;28msetattr[39m([38;5;28mself[39m, name, namespace)
[1;32m    192[0m     [38;5;28;01mreturn[39;00m namespace

[0;31mRuntimeError[0m: No such operator aten::_s_where
RuntimeError: No such operator aten::_s_where

