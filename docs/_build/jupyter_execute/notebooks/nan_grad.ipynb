{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b35d951",
   "metadata": {},
   "source": [
    "# Resolving NaN Grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e997f94",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/pytorch/maskedtensor/tree/main/docs/notebooks/nan_grad.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e304e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "if \"1.11.0\" not in torch.__version__:\n",
    "    !pip uninstall --y torch\n",
    "    !pip install torch -f https://download.pytorch.org/whl/test/cu102/torch_test.html --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a46db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/\r\n",
      "Requirement already satisfied: maskedtensor in /data/home/georgeqi/miniconda/envs/mt_release_0.1.0/lib/python3.8/site-packages (0.1.0)\r\n",
      "Requirement already satisfied: torch<1.12,>=1.11 in /data/home/georgeqi/miniconda/envs/mt_release_0.1.0/lib/python3.8/site-packages (from maskedtensor) (1.11.0+cpu)\r\n",
      "Requirement already satisfied: typing-extensions in /data/home/georgeqi/miniconda/envs/mt_release_0.1.0/lib/python3.8/site-packages (from torch<1.12,>=1.11->maskedtensor) (3.10.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ maskedtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d81a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import factory function\n",
    "from maskedtensor import masked_tensor\n",
    "from maskedtensor import as_masked_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc28539",
   "metadata": {},
   "source": [
    "## Resolving Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4373911",
   "metadata": {},
   "source": [
    "One issue that vanilla tensors run into is the inability to differentiate between gradients that are not defined (nan) vs. gradients that are actually 0.\n",
    "\n",
    "Below, we show by example several different issues where MaskedTensor can resolve and/or work around these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959083c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [10729 - torch.where](https://github.com/pytorch/pytorch/issues/10729)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e08d51",
   "metadata": {},
   "source": [
    "**PyTorch result**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8f3fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([4.5400e-05, 6.7379e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "       grad_fn=<SWhereBackward0>)\n",
      "x.grad: tensor([4.5400e-05, 6.7379e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00,        nan,        nan])\n",
      "y.grad: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/georgeqi/miniconda/envs/mt_release_0.1.0/lib/python3.8/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# This behavior underlies the fix to clamp, which uses where in its derivative\n",
    "x = torch.tensor([-10., -5, 0, 5, 10, 50, 60, 70, 80, 90, 100], requires_grad=True)\n",
    "y = torch.where(x < 0, torch.exp(x), torch.ones_like(x))\n",
    "print(\"y:\", y)\n",
    "y.sum().backward()\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb5976",
   "metadata": {},
   "source": [
    "**MaskedTensor result**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff33802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mx.grad:  masked_tensor(\n",
      "  [  0.0000,   0.0067,       --,       --,       --,       --,       --,       --,       --,       --,       --]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-10., -5, 0, 5, 10, 50, 60, 70, 80, 90, 100], requires_grad=True)\n",
    "mask = x < 0\n",
    "mx = masked_tensor(x, mask, requires_grad=True)\n",
    "my = masked_tensor(torch.ones_like(x), ~mask, requires_grad=True)\n",
    "y = torch.where(mask, torch.exp(mx), my)\n",
    "s = y.sum()\n",
    "s.backward()\n",
    "# Gradient is only provided to selected subset.\n",
    "# Effectively this changes the gradient of where to mask out elements instead\n",
    "# of setting them to zero.\n",
    "print(\"mx.grad: \", mx.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcdce44",
   "metadata": {},
   "source": [
    "The gradient here is only provided to the selected subset. Effectively, this changes the gradient of where to mask out elements instead of setting them to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c1a45",
   "metadata": {},
   "source": [
    "### [52248 - another torch.where](https://github.com/pytorch/pytorch/issues/52248)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ac25d",
   "metadata": {},
   "source": [
    "**PyTorch result**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d5915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<SWhereBackward0>)\n",
      "(tensor(nan),)\n"
     ]
    }
   ],
   "source": [
    "# A more recent incarnation specific to where of this\n",
    "# https://github.com/pytorch/pytorch/issues/52248\n",
    "\n",
    "a = torch.randn((), requires_grad=True)\n",
    "b = torch.tensor(False)\n",
    "c = torch.ones(())\n",
    "\n",
    "print(torch.where(b, a/0, c))\n",
    "print(torch.autograd.grad(torch.where(b, a/0, c), a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed2500",
   "metadata": {},
   "source": [
    "**MaskedTensor result**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9019e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_tensor(  1.0000, True)\n",
      "(masked_tensor(--, False),)\n"
     ]
    }
   ],
   "source": [
    "a = masked_tensor(torch.randn(()), torch.tensor(True), requires_grad=True)\n",
    "b = torch.tensor(False)\n",
    "c = torch.ones(())\n",
    "\n",
    "print(torch.where(b, a/0, c))\n",
    "print(torch.autograd.grad(torch.where(b, a/0, c), a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2a2ec",
   "metadata": {},
   "source": [
    "### [67180 - torch.nansum and torch.nanmean](https://github.com/pytorch/pytorch/issues/67180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b93038",
   "metadata": {},
   "source": [
    "**PyTorch result**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6bb852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1., 2., float('nan')])\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "c = a * b\n",
    "c1 = torch.nansum(c)  # or torch.nanmean\n",
    "\n",
    "bgrad1, = torch.autograd.grad(c1, b, retain_graph=True)\n",
    "bgrad1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b32e7",
   "metadata": {},
   "source": [
    "**MaskedTensor result**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41df5949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_tensor(  3.0000, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1., 2., float('nan')])\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "ma = masked_tensor(a, ~torch.isnan(a))\n",
    "c = ma * b\n",
    "c1 = torch.sum(c)  # or torch.nanmean\n",
    "\n",
    "bgrad1, = torch.autograd.grad(c1, b, retain_graph=True)\n",
    "bgrad1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa8f33",
   "metadata": {},
   "source": [
    "### [4132 - when using mask, x/0 yields NaN grad](https://github.com/pytorch/pytorch/issues/4132)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a65cd",
   "metadata": {},
   "source": [
    "PyTorch result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95aff964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 1.], requires_grad=True)\n",
    "div = torch.tensor([0., 1.])\n",
    "y = x/div # => y is [inf, 1]\n",
    "\n",
    "mask = (div != 0) # => mask is [0, 1]\n",
    "loss = y[mask]\n",
    "loss.backward()\n",
    "\n",
    "x.grad # grad is [nan, 1], but expected [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f7ce9",
   "metadata": {},
   "source": [
    "MaskedTensor result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "833b1f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_tensor(\n",
       "  [      --,   1.0000]\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 1.], requires_grad=True)\n",
    "div = torch.tensor([0., 1.])\n",
    "y = x/div # => y is [inf, 1]\n",
    "\n",
    "mask = (div != 0) # => mask is [0, 1]\n",
    "loss = as_masked_tensor(y, mask)\n",
    "# We could add autograd support for indexing here instead of using sum\n",
    "loss = loss.sum()\n",
    "loss.backward()\n",
    "\n",
    "x.grad # grad is [nan, 1], but expected [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46865039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "source_map": [
   13,
   17,
   21,
   29,
   33,
   37,
   41,
   47,
   51,
   55,
   63,
   67,
   79,
   83,
   87,
   91,
   101,
   105,
   112,
   116,
   120,
   128,
   132,
   141,
   145,
   149,
   159,
   163,
   177
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}